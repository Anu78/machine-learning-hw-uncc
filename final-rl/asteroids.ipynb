{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60e618f8-127d-47f6-8531-9677ea431478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "env = gym.make(\"ALE/Asteroids-v5\")\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "print(env.observation_space.shape)\n",
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0855472a-72dc-4ec9-985e-64429e1a99fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f1f6c43-c194-45c3-af52-937a0d8c986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        # Defining the convolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Input: 3x160x210\n",
    "            nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=2),  # Output: 16x160x210\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),  # Output: 16x80x105\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2), # Output: 32x80x105\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),  # Output: 32x40x52\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2), # Output: 64x40x52\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)   # Output: 64x20x26\n",
    "        )\n",
    "        \n",
    "        # Calculating the total number of features after the final pooling layer\n",
    "        # Output dimensions are 64 channels, each with a size of 20x26\n",
    "        self.num_features = 64 * 20 * 26\n",
    "        \n",
    "        # Defining the fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.num_features, 256),  # from calculated features to 256\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 14)  # 256 inputs to 14 outputs (actions)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(-1, self.num_features)  # Manually flatten to ensure correct shape\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ec6d90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    global steps_done\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-steps_done / EPS_DECAY)\n",
    "    if random.random() > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # Ensure action is a 1D tensor with batch dimension\n",
    "            return policy_model(state).argmax(dim=1, keepdim=True).to(device)\n",
    "    else:\n",
    "        # Randomly sample an action and ensure it's also a 1D tensor\n",
    "        return torch.tensor([[random.randrange(env.action_space.n)]], device=device, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6360ecff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s.unsqueeze(0) for s in batch.next_state if s is not None], dim=0)\n",
    "\n",
    "    state_batch = torch.cat([s.unsqueeze(0) for s in batch.state], dim=0)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "\n",
    "    state_action_values = policy_model(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        if len(non_final_next_states) > 0:\n",
    "            next_state_values[non_final_mask] = target_model(non_final_next_states).max(1).values\n",
    "\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(policy_model.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19c214ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 0.05\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "UPDATE_TARGET_EVERY = 2800\n",
    "\n",
    "# initialize things\n",
    "memory = ReplayMemory(10000)\n",
    "policy_model = DQN().to(device)\n",
    "target_model = DQN().to(device)\n",
    "target_model.load_state_dict(policy_model.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_model.parameters(), lr=LR, amsgrad=True)\n",
    " \n",
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d5d1b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(path):\n",
    "    torch.save(policy_model.state_dict(), path)\n",
    "\n",
    "def load_model(path):\n",
    "    policy_model.load_state_dict(torch.load(path))\n",
    "    policy_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0dc9438a-38bf-45f4-9923-94ee36328d2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m current_lives \u001b[38;5;241m=\u001b[39m info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlives\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;66;03m# intialize # of lives from environment\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m count():\n\u001b[0;32m---> 12\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlives\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m current_lives:\n",
      "Cell \u001b[0;32mIn[28], line 10\u001b[0m, in \u001b[0;36mselect_action\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m policy_model(state)\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Randomly sample an action and ensure it's also a 1D tensor\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandrange\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 50\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state,dtype=torch.float32,device=device).permute(2,1,0)\n",
    "    current_lives = info[\"lives\"] # intialize # of lives from environment\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, info = env.step(action.item())\n",
    "\n",
    "        if info['lives'] < current_lives:\n",
    "            reward -= 100\n",
    "            current_lives = info['lives']  \n",
    "\n",
    "        reward += 1  \n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).permute(2,1,0)\n",
    "\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        optimize_model()\n",
    "\n",
    "        if steps_done % UPDATE_TARGET_EVERY == 0:\n",
    "            # update target network using tau\n",
    "            target_net_state_dict = target_model.state_dict()\n",
    "            policy_net_state_dict = policy_model.state_dict()\n",
    "            for key in policy_net_state_dict:\n",
    "                target_net_state_dict[key] = policy_net_state_dict[key] * TAU + target_net_state_dict[key] * (1 - TAU)\n",
    "            target_model.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            print(f\"on episode {i_episode}, which lasted {t} frames\")\n",
    "            break\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c96fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(target_model.state_dict(), \"./rl-asteroids-v1-target.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
